{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import gc\n",
    "import sys\n",
    "path = os.path.join(os.getcwd(), '..')\n",
    "sys.path.append(path)\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from ensemble_boxes import *\n",
    "\n",
    "from src.config.GlobalConfig import GlobalConfig\n",
    "from src.model.efficientdet import get_efficientdet\n",
    "from src.data.df_utils import read_boxes_df, get_kfolds_df\n",
    "from src.common.utils import read_image, xywh2ltrb\n",
    "\n",
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = '/userhome/34/h3509807/wheat-data/train.csv'\n",
    "DATA_ROOT_PATH = '/userhome/34/h3509807/wheat-data'\n",
    "MODEL_CKPT = '../models/effdet5-512/best-checkpoint-256.bin'\n",
    "\n",
    "RESIZE_SZ = 512\n",
    "SCALE_FACTOR = int(1024 / RESIZE_SZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dataset and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/31/h3509807/anaconda3/envs/fastai2/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "boxes_df = read_boxes_df(CSV_PATH)\n",
    "kfolds_df = get_kfolds_df(boxes_df, kfolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(height = RESIZE_SZ, width = RESIZE_SZ, p = 1.0),\n",
    "        ToTensorV2(p = 1.)\n",
    "        ], p = 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = kfolds_df[kfolds_df.fold == 0].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kFoldsDataset(Dataset):\n",
    "    def __init__(self, boxes_df, kfolds_df, kfolds_idx, \n",
    "                 train_dir, transforms = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_ids = kfolds_df[kfolds_df.fold == kfolds_idx].index.values\n",
    "        self.boxes_df = boxes_df\n",
    "        self.transforms = transforms\n",
    "        self.train_dir = train_dir\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        image, boxes = self.load_image_and_boxes(index)\n",
    "        \n",
    "        # only one class\n",
    "        labels = torch.ones((boxes.shape[0],), dtype = torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['index'] = torch.tensor([index])\n",
    "\n",
    "        if self.transforms:\n",
    "            for i in range(10):\n",
    "                sample = self.transforms(**{\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                })\n",
    "\n",
    "                if len(sample['bboxes']) > 0:\n",
    "                    image = sample['image']\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                    #yxyx: be warning\n",
    "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  \n",
    "                    target['labels'] = torch.as_tensor(sample['labels'])\n",
    "                    break\n",
    "                    \n",
    "        assert target['boxes'].shape[0] ==target['labels'].shape[0], 'boxes len != labels len'\n",
    "        return image, target, image_id\n",
    "    \n",
    "    def load_image_and_boxes(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = read_image(image_id, self.train_dir)\n",
    "        records = self.boxes_df[self.boxes_df['image_id'] == image_id]\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes = xywh2ltrb(boxes)\n",
    "        return image, boxes\n",
    "    \n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = f'{DATA_ROOT_PATH}/train'\n",
    "tfms = get_valid_transforms()\n",
    "\n",
    "ds = kFoldsDataset(boxes_df, kfolds_df, kfolds_idx = 0, \n",
    "                   train_dir = train_dir, transforms = tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size = 2, shuffle = False, \n",
    "                num_workers = 4, drop_last = False, \n",
    "                collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
