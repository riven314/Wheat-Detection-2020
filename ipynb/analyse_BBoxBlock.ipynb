{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demifies BBoxBlock. \n",
    "\n",
    "### Key Questions\n",
    "1. After BBoxBlock transformation of my pipeline, what is the format of the bbox?\n",
    "    - It depends on your bbox format before feeding into BBoxBlock\n",
    "    - ```get_annotations``` output bbox in [x0, y0, x1, y1] format\n",
    "\n",
    "\n",
    "### Notes\n",
    "1. When ```Transform``` intake a transform function, no instantiation is needed. If you do subclass on ```Transform```, then you need instantiation.\n",
    "2. In ```PointScaler.__init__```, there is an argument ```y_first``` indicate the input bbox format\n",
    "3. ```PointScaler.encode``` apply ```_scale_pnts```, ```y_first``` will flip input from [y0, x0, y1, x1] to [x0, y0, x1, y1]\n",
    "4. ```bb_pad``` takes list of tuple (each tuple represents an image, bbox and its label), take a max bbox # among the list of samples. And apply padding on bbox and label for the rest of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path = os.path.join(os.getcwd(), '..')\n",
    "sys.path.append(path)\n",
    "\n",
    "from pathlib import Path\n",
    "from pdb import set_trace\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from fastai2.vision.all import *\n",
    "\n",
    "from src.data.dblock import build_dblock\n",
    "from src.data.dls import build_dataloaders\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Wrap a Function into ```Transform```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbl = Transform(double)\n",
    "dbl(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BBoxBlock\n",
    "```\n",
    "BBoxBlock = TransformBlock(  \n",
    "    type_tfms=TensorBBox.create,   \n",
    "    item_tfms=PointScaler,   \n",
    "    dls_kwargs = {'before_batch': bb_pad}  \n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. type_tfms: TensorBBox.create (function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_bbox = [10., 20., 30., 40.]\n",
    "b_bbox = TensorBBox.create([10., 20., 30., 40.])\n",
    "b_bbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBBox([[10., 20., 30., 40.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. item_tfms: PointScaler (Transform class)\n",
    "```\n",
    "class PointScaler(Transform):\n",
    "    \"Scale a tensor representing points\"\n",
    "    order = 1\n",
    "    def __init__(self, do_scale=True, y_first=False): \n",
    "        self.do_scale,self.y_first = do_scale,y_first\n",
    "    def _grab_sz(self, x):\n",
    "        if isinstance(x, Tensor):\n",
    "            self.sz = [x.shape[-1], x.shape[-2]]  \n",
    "        else: \n",
    "            x.size\n",
    "        return x\n",
    "```\n",
    "\n",
    "#### Notes\n",
    "Essentially PointScaler do 2 things when transforming ```TensorBBox```:\n",
    "1. it horizontally flip ```TensorBBox``` if ```y_first = True```\n",
    "2. it rescale bbox such that center = (0, 0), and bbox ranges = [-1, +1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fastai2.vision.core.TensorBBox, True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b_bbox), isinstance(b_bbox, TensorPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBBox([[10., 20., 30., 40.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_bbox.sz = 100\n",
    "b_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_bbox.get_meta('img_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorBBox([[10., 20., 30., 40.]]), TensorBBox([[20., 10., 40., 30.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PointScaler.sz = 256\n",
    "PointScaler(do_scale = False)(b_bbox), PointScaler(do_scale = False, y_first = True)(b_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PointScaler() does the following:\n",
    "1. Do not flip (x, y) coordinate\n",
    "2. Apply scaling so that image center = [0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBBox([[-0.9219, -0.8438, -0.7656, -0.6875]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_bbox = PointScaler()(b_bbox)\n",
    "c_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. batch transform: bb_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(TensorImage([[[ 2.1758, -0.0153,  0.6612],\n",
       "           [ 0.2474, -0.0908,  0.7472]],\n",
       "  \n",
       "          [[ 0.6527,  0.2712,  1.7468],\n",
       "           [ 0.2420, -1.5194, -1.2021]]]),\n",
       "  TensorBBox([[-0.9219, -0.8438, -0.7656, -0.6875]]),\n",
       "  tensor([1.])),\n",
       " (TensorImage([[[ 2.1758, -0.0153,  0.6612],\n",
       "           [ 0.2474, -0.0908,  0.7472]],\n",
       "  \n",
       "          [[ 0.6527,  0.2712,  1.7468],\n",
       "           [ 0.2420, -1.5194, -1.2021]]]),\n",
       "  tensor([[-0.9219, -0.8438, -0.7656, -0.6875],\n",
       "          [-0.9219, -0.8438, -0.7656, -0.6875],\n",
       "          [-0.9219, -0.8438, -0.7656, -0.6875]]),\n",
       "  tensor([1., 1., 1.]))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = TensorImage(torch.randn((2, 2, 3)))\n",
    "bbox = c_bbox\n",
    "lbl = Tensor([1])\n",
    "\n",
    "ccc_bbox = torch.stack((bbox, bbox, bbox)).squeeze()\n",
    "ccc_lbl = Tensor([1, 1, 1])\n",
    "\n",
    "sample1 = (img, c_bbox, lbl)\n",
    "sample2 = (img, ccc_bbox, ccc_lbl)\n",
    "samples = [sample1, sample2]\n",
    "\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(TensorImage([[[ 2.1758, -0.0153,  0.6612],\n",
       "           [ 0.2474, -0.0908,  0.7472]],\n",
       "  \n",
       "          [[ 0.6527,  0.2712,  1.7468],\n",
       "           [ 0.2420, -1.5194, -1.2021]]]),\n",
       "  tensor([[-0.9219, -0.8438, -0.7656, -0.6875],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]]),\n",
       "  tensor([1., 0., 0.])),\n",
       " (TensorImage([[[ 2.1758, -0.0153,  0.6612],\n",
       "           [ 0.2474, -0.0908,  0.7472]],\n",
       "  \n",
       "          [[ 0.6527,  0.2712,  1.7468],\n",
       "           [ 0.2420, -1.5194, -1.2021]]]),\n",
       "  tensor([[-0.9219, -0.8438, -0.7656, -0.6875],\n",
       "          [-0.9219, -0.8438, -0.7656, -0.6875],\n",
       "          [-0.9219, -0.8438, -0.7656, -0.6875]]),\n",
       "  tensor([1., 1., 1.]))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_pad(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How I read BBox before Feeding into BBoxBlock?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.utils import decode_coco_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7) [Path('/userhome/34/h3509807/wheat-data/train.json'),Path('/userhome/34/h3509807/wheat-data/bkup'),Path('/userhome/34/h3509807/wheat-data/train_mini.json'),Path('/userhome/34/h3509807/wheat-data/train'),Path('/userhome/34/h3509807/wheat-data/train.csv'),Path('/userhome/34/h3509807/wheat-data/sample_submission.csv'),Path('/userhome/34/h3509807/wheat-data/test')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path('/userhome/34/h3509807/wheat-data')\n",
    "data_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids, lbl_bbox, img2bbox = decode_coco_json(data_path / 'train_mini.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[834.0, 222.0, 890.0, 258.0],\n",
       "  [226.0, 548.0, 356.0, 606.0],\n",
       "  [377.0, 504.0, 451.0, 664.0],\n",
       "  [834.0, 95.0, 943.0, 202.0],\n",
       "  [26.0, 144.0, 150.0, 261.0],\n",
       "  [569.0, 382.0, 688.0, 493.0],\n",
       "  [52.0, 602.0, 134.0, 647.0],\n",
       "  [627.0, 302.0, 749.0, 377.0],\n",
       "  [412.0, 367.0, 480.0, 449.0],\n",
       "  [953.0, 220.0, 1009.0, 323.0],\n",
       "  [30.0, 70.0, 156.0, 203.0],\n",
       "  [35.0, 541.0, 81.0, 587.0],\n",
       "  [103.0, 60.0, 220.0, 143.0],\n",
       "  [417.0, 4.0, 527.0, 95.0],\n",
       "  [764.0, 299.0, 883.0, 392.0],\n",
       "  [539.0, 58.0, 597.0, 188.0],\n",
       "  [139.0, 274.0, 260.0, 350.0],\n",
       "  [461.0, 634.0, 579.0, 698.0],\n",
       "  [215.0, 634.0, 328.0, 709.0],\n",
       "  [134.0, 903.0, 261.0, 952.0],\n",
       "  [737.0, 545.0, 824.0, 593.0],\n",
       "  [292.0, 930.0, 335.0, 976.0],\n",
       "  [0.0, 827.0, 86.0, 885.0],\n",
       "  [324.0, 44.0, 381.0, 114.0],\n",
       "  [663.0, 794.0, 779.0, 858.0],\n",
       "  [325.0, 730.0, 401.0, 802.0],\n",
       "  [155.0, 554.0, 229.0, 624.0],\n",
       "  [783.0, 833.0, 853.0, 924.0],\n",
       "  [534.0, 46.0, 607.0, 270.0],\n",
       "  [155.0, 281.0, 261.0, 419.0],\n",
       "  [101.0, 240.0, 183.0, 315.0],\n",
       "  [583.0, 329.0, 663.0, 412.0],\n",
       "  [36.0, 595.0, 128.0, 636.0],\n",
       "  [0.0, 487.0, 46.0, 558.0],\n",
       "  [25.0, 482.0, 76.0, 542.0],\n",
       "  [160.0, 689.0, 300.0, 763.0],\n",
       "  [202.0, 702.0, 306.0, 786.0],\n",
       "  [852.0, 677.0, 967.0, 835.0],\n",
       "  [904.0, 700.0, 977.0, 820.0],\n",
       "  [175.0, 866.0, 257.0, 909.0],\n",
       "  [612.0, 882.0, 827.0, 1018.0],\n",
       "  [675.0, 986.0, 806.0, 1024.0],\n",
       "  [959.0, 869.0, 982.0, 900.0],\n",
       "  [600.0, 723.0, 821.0, 804.0],\n",
       "  [575.0, 146.0, 647.0, 267.0],\n",
       "  [945.0, 193.0, 1002.0, 238.0],\n",
       "  [216.0, 440.0, 262.0, 460.0]],\n",
       " ['wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat',\n",
       "  'wheat'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2bbox[img_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
